{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w0m1M11eKPpw",
        "outputId": "f48cb450-2041-4185-8f86-474b14da4c1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.6)\n",
            "Requirement already satisfied: pdfminer.six==20250327 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250327)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install pdfplumber\n",
        "import logging\n",
        "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "g8-2vomw-Rqr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import pipeline, AutoModel, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import pdfplumber\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "OYtkrC1MMcPj"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load and Preprocess the Book\n",
        "def load_book(filepath):\n",
        "    \"\"\"Reads a text or PDF file and returns its content as a string.\"\"\"\n",
        "    if filepath.endswith(\".txt\"):\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            return f.read()\n",
        "    elif filepath.endswith(\".pdf\"):\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(filepath) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                # Get the MediaBox and crop the text based on it\n",
        "                mediabox = page.mediabox\n",
        "                text += page.crop(mediabox).extract_text() + \"\\n\"  # Added crop section\n",
        "        return text\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Use .txt or .pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kc782h98Mau3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 2: Split Text into Chunks\n",
        "def split_text(text, chunk_size=500):\n",
        "    \"\"\"Splits text into smaller chunks for retrieval.\"\"\"\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "collapsed": true,
        "id": "Qaqwh0YBMRDD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 3: Convert Text Chunks into Embeddings\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "def create_embeddings(chunks):\n",
        "    return embedding_model.encode(chunks, convert_to_numpy=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "NhDDU59EMNnF"
      },
      "outputs": [],
      "source": [
        "# Step 4: Store Embeddings in FAISS Index\n",
        "def build_faiss_index(embeddings):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    return index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "V1cbPtHMMJht"
      },
      "outputs": [],
      "source": [
        "# Step 5: Retrieve Relevant Chunks\n",
        "def retrieve_passage(question, chunks, index, top_k=3):\n",
        "    query_vector = embedding_model.encode([question])\n",
        "    distances, indices = index.search(query_vector, top_k)\n",
        "    return \" \".join([chunks[i] for i in indices[0]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d72D90JEMBCK",
        "outputId": "3120571c-bc44-4b72-ad63-5a17f07d516b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Load a Question Answering Model\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "\n",
        "def answer_question(question, context):\n",
        "    return qa_pipeline(question=question, context=context)[\"answer\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6J66vKDLueJ",
        "outputId": "7368b309-6cae-405d-eb17-15220814f42a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: who is kalam.?\n",
            "A: one of the most distinguished scientists of India\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Run the Model\n",
        "if __name__ == \"__main__\":\n",
        "    book_text = load_book(r\"/content/Dr._Kalam-profile.pdf\")\n",
        "    chunks = split_text(book_text)\n",
        "    embeddings = create_embeddings(chunks)\n",
        "    faiss_index = build_faiss_index(embeddings)\n",
        "\n",
        "    question = \"who is kalam.?\"\n",
        "    retrieved_context = retrieve_passage(question, chunks, faiss_index)\n",
        "    answer = answer_question(question, retrieved_context)\n",
        "\n",
        "    print(f\"Q: {question}\\nA: {answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "0fPSa8_XNxYi"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import pickle\n",
        "\n",
        "# Save the FAISS index\n",
        "faiss.write_index(faiss_index, \"faiss_index.bin\")\n",
        "\n",
        "# Save the chunks (as you'll need them later)\n",
        "with open(\"chunks.pkl\", \"wb\") as f:\n",
        "    pickle.dump(chunks, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "j2hPG5zXti_A"
      },
      "outputs": [],
      "source": [
        "embedding_model.save(\"embedding_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fORVZjmItlsQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}